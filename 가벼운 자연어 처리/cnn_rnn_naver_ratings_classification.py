# -*- coding: utf-8 -*-
"""cnn-rnn_naver-ratings_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pe_vsGNgLvVl_yx0torA9Vx_UVI1YsR5
"""

!set -x \
&& pip install konlpy \
&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

!wget "https://drive.google.com/uc?export=download&id=1ByJN0Vh4ctIwNdnO2jevEcBgrbRHuNyM" -O "ratings_train.txt"
!wget "https://drive.google.com/uc?export=download&id=1fNm-8pQJsuDbFaVIMow1DI-7lsnNLRFB" -O "ratings_test.txt"

train_data = pd.read_table('ratings_train.txt')
test_data = pd.read_table('ratings_test.txt')

train_data.head()

len(train_data)

from konlpy.tag import Mecab

mecab  = Mecab()

X_train = [mecab.morphs(sentence) for sentence in train_data['document']]

X_test = [mecab.morphs(sentence) for sentence in test_data['document']]

X_train[:3]#띄어쓰기를 위한 전처리 도구가 있지만 다음 시간에

vocab_size = 21645

tokenizer = Tokenizer(vocab_size, oov_token='OOV') 
tokenizer.fit_on_texts(X_train)

tokenizer.word_index

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

X_train[:3]

y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])

y_train[:3]

drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]

len(drop_train)

drop_train

train_data.loc[[404,412,470], ['document']]

train_data.head(22314)

# 빈 샘플들을 제거
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0)

len(X_train)

max_len = 80

X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

X_train.shape, X_test.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dropout, Conv1D, MaxPooling1D, Dense, LSTM

embedding_dim = 128
batch_size = 256

model =Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))
model.add(Dropout(0.3))
model.add(Conv1D(128, 3, padding='valid', activation= 'relu'))
model.add(MaxPooling1D(3))
model.add(LSTM(64, dropout=0.1))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

21645*128

model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model, show_shapes=True)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(x=X_train, y=y_train,
                    epochs=20,
                    batch_size=batch_size,
                    validation_split=0.2)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

test_loss, test_accuracy = model.evaluate(X_test, y_test)

