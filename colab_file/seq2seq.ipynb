{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2pJLBHgF5GH",
        "outputId": "2d2fd823-0e67-4f0a-e080-99ced7a6dc71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=1rc_3n91So-S-zyoRh3JD9SoeJoU15TbM\" -O \"fra.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-06 06:41:42--  https://drive.google.com/uc?export=download&id=1rc_3n91So-S-zyoRh3JD9SoeJoU15TbM\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.139, 74.125.142.100, 74.125.142.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/grsv18tjeeijfrdppdaad9vrf3ta1ii2/1604644875000/13472183013488020071/*/1rc_3n91So-S-zyoRh3JD9SoeJoU15TbM?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-11-06 06:41:42--  https://doc-14-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/grsv18tjeeijfrdppdaad9vrf3ta1ii2/1604644875000/13472183013488020071/*/1rc_3n91So-S-zyoRh3JD9SoeJoU15TbM?e=download\n",
            "Resolving doc-14-94-docs.googleusercontent.com (doc-14-94-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-14-94-docs.googleusercontent.com (doc-14-94-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘fra.txt’\n",
            "\n",
            "fra.txt                 [  <=>               ]  11.33M  52.2MB/s    in 0.2s    \n",
            "\n",
            "2020-11-06 06:41:42 (52.2 MB/s) - ‘fra.txt’ saved [11876774]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhrA4TUtGAg0"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNGtUuS3GPJd"
      },
      "source": [
        "batch_size = 64      # 학습 배치 사이즈.\n",
        "epochs = 100         # 학습하고자 하는 에폭 사이즈.\n",
        "latent_dim = 256     # 인코더 디코더 LSTM 모델의 unit 사이즈.\n",
        "num_samples = 10000  # 입력 데이터의 최대 sequence.\n",
        "# 프랑스-영어 병렬 코퍼스 파일 경로.\n",
        "data_path = 'fra.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9PuqiDJGQ-b"
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # \"tab\"을 목표 데이터의 시작, 종료 문자로 지정\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGENZvuhH1rk"
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT0l7ZGoJAbN"
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkJFr4nLMrLt"
      },
      "source": [
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jScXKtRjKOIU"
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # 디코더의 목표 데이터는 디코더 입력 데이터 보다 한 step 만큼 앞서 있음\n",
        "            # 또한 디코더의 목표 데이터는 시작 문자(tab) 이 존재하지 않음\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNBT2BirLPXD"
      },
      "source": [
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True, name='encoder')\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# 인코더의 결과 값 중 맥락벡터(h, c) 만 디코더 모델로 전달\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM_f5vOmPhj4"
      },
      "source": [
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# 디코더의 상태값(h, c)값을 매 스텝마다 리턴 받도록 설정\n",
        "# 학습시에는 상태값을 사용하지 않지만 예측 시에 사용\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder')\n",
        "# 인코더에서 계산 된 맥락벡터(h, c)를 디코더의 첫번째 은닉층의 상태값으로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mILzqS6zQX--"
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy0sKwOsQf__",
        "outputId": "6a74f193-4e23-4755-8cb8-1fc41b7a7f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "plot_model(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t492Va1QiAe"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gWrnxshRXvi",
        "outputId": "6a6a4c27-b3bf-4d46-b937-01d35629cecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          epochs=epochs, batch_size=batch_size, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.9163 - accuracy: 0.0652 - val_loss: 0.9505 - val_accuracy: 0.0947\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.7346 - accuracy: 0.1143 - val_loss: 0.7674 - val_accuracy: 0.1358\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.6210 - accuracy: 0.1388 - val_loss: 0.6782 - val_accuracy: 0.1571\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5632 - accuracy: 0.1534 - val_loss: 0.6366 - val_accuracy: 0.1681\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.5233 - accuracy: 0.1644 - val_loss: 0.5969 - val_accuracy: 0.1809\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4924 - accuracy: 0.1727 - val_loss: 0.5723 - val_accuracy: 0.1876\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4667 - accuracy: 0.1798 - val_loss: 0.5482 - val_accuracy: 0.1952\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4445 - accuracy: 0.1855 - val_loss: 0.5259 - val_accuracy: 0.2005\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4255 - accuracy: 0.1903 - val_loss: 0.5162 - val_accuracy: 0.2038\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.4085 - accuracy: 0.1956 - val_loss: 0.5062 - val_accuracy: 0.2055\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3928 - accuracy: 0.1998 - val_loss: 0.4902 - val_accuracy: 0.2101\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3786 - accuracy: 0.2042 - val_loss: 0.4805 - val_accuracy: 0.2132\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3647 - accuracy: 0.2084 - val_loss: 0.4756 - val_accuracy: 0.2140\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3518 - accuracy: 0.2119 - val_loss: 0.4674 - val_accuracy: 0.2178\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3396 - accuracy: 0.2157 - val_loss: 0.4628 - val_accuracy: 0.2188\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3279 - accuracy: 0.2189 - val_loss: 0.4567 - val_accuracy: 0.2208\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3170 - accuracy: 0.2223 - val_loss: 0.4533 - val_accuracy: 0.2231\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.3071 - accuracy: 0.2255 - val_loss: 0.4521 - val_accuracy: 0.2234\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2970 - accuracy: 0.2283 - val_loss: 0.4528 - val_accuracy: 0.2237\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2875 - accuracy: 0.2307 - val_loss: 0.4506 - val_accuracy: 0.2248\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2786 - accuracy: 0.2336 - val_loss: 0.4474 - val_accuracy: 0.2261\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2697 - accuracy: 0.2360 - val_loss: 0.4485 - val_accuracy: 0.2265\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2613 - accuracy: 0.2390 - val_loss: 0.4484 - val_accuracy: 0.2268\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2536 - accuracy: 0.2409 - val_loss: 0.4464 - val_accuracy: 0.2277\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2455 - accuracy: 0.2434 - val_loss: 0.4490 - val_accuracy: 0.2275\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2381 - accuracy: 0.2458 - val_loss: 0.4538 - val_accuracy: 0.2276\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2310 - accuracy: 0.2476 - val_loss: 0.4544 - val_accuracy: 0.2280\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2243 - accuracy: 0.2494 - val_loss: 0.4552 - val_accuracy: 0.2284\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2177 - accuracy: 0.2515 - val_loss: 0.4594 - val_accuracy: 0.2282\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2115 - accuracy: 0.2537 - val_loss: 0.4588 - val_accuracy: 0.2290\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2060 - accuracy: 0.2549 - val_loss: 0.4647 - val_accuracy: 0.2286\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.2001 - accuracy: 0.2568 - val_loss: 0.4671 - val_accuracy: 0.2289\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1942 - accuracy: 0.2583 - val_loss: 0.4689 - val_accuracy: 0.2291\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1888 - accuracy: 0.2602 - val_loss: 0.4779 - val_accuracy: 0.2281\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1836 - accuracy: 0.2618 - val_loss: 0.4790 - val_accuracy: 0.2287\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1786 - accuracy: 0.2632 - val_loss: 0.4822 - val_accuracy: 0.2295\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1739 - accuracy: 0.2645 - val_loss: 0.4921 - val_accuracy: 0.2281\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1694 - accuracy: 0.2658 - val_loss: 0.4910 - val_accuracy: 0.2285\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1650 - accuracy: 0.2670 - val_loss: 0.4931 - val_accuracy: 0.2297\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1606 - accuracy: 0.2684 - val_loss: 0.5012 - val_accuracy: 0.2277\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1563 - accuracy: 0.2696 - val_loss: 0.5039 - val_accuracy: 0.2289\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1525 - accuracy: 0.2707 - val_loss: 0.5044 - val_accuracy: 0.2292\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1487 - accuracy: 0.2720 - val_loss: 0.5156 - val_accuracy: 0.2279\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1451 - accuracy: 0.2732 - val_loss: 0.5160 - val_accuracy: 0.2287\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1415 - accuracy: 0.2740 - val_loss: 0.5206 - val_accuracy: 0.2286\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1382 - accuracy: 0.2751 - val_loss: 0.5216 - val_accuracy: 0.2297\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1350 - accuracy: 0.2761 - val_loss: 0.5289 - val_accuracy: 0.2285\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1315 - accuracy: 0.2769 - val_loss: 0.5349 - val_accuracy: 0.2270\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1283 - accuracy: 0.2780 - val_loss: 0.5411 - val_accuracy: 0.2284\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1256 - accuracy: 0.2788 - val_loss: 0.5446 - val_accuracy: 0.2274\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1226 - accuracy: 0.2798 - val_loss: 0.5503 - val_accuracy: 0.2272\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1200 - accuracy: 0.2804 - val_loss: 0.5519 - val_accuracy: 0.2287\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1176 - accuracy: 0.2811 - val_loss: 0.5528 - val_accuracy: 0.2289\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1144 - accuracy: 0.2821 - val_loss: 0.5683 - val_accuracy: 0.2272\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1124 - accuracy: 0.2826 - val_loss: 0.5663 - val_accuracy: 0.2273\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1098 - accuracy: 0.2837 - val_loss: 0.5698 - val_accuracy: 0.2274\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1077 - accuracy: 0.2841 - val_loss: 0.5721 - val_accuracy: 0.2277\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1053 - accuracy: 0.2847 - val_loss: 0.5764 - val_accuracy: 0.2278\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1031 - accuracy: 0.2854 - val_loss: 0.5842 - val_accuracy: 0.2264\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.1009 - accuracy: 0.2862 - val_loss: 0.5859 - val_accuracy: 0.2283\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0989 - accuracy: 0.2866 - val_loss: 0.5888 - val_accuracy: 0.2275\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0968 - accuracy: 0.2872 - val_loss: 0.6035 - val_accuracy: 0.2269\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0950 - accuracy: 0.2878 - val_loss: 0.6016 - val_accuracy: 0.2270\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0932 - accuracy: 0.2883 - val_loss: 0.6077 - val_accuracy: 0.2271\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0912 - accuracy: 0.2886 - val_loss: 0.6128 - val_accuracy: 0.2267\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0896 - accuracy: 0.2892 - val_loss: 0.6182 - val_accuracy: 0.2272\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0881 - accuracy: 0.2896 - val_loss: 0.6227 - val_accuracy: 0.2267\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0863 - accuracy: 0.2902 - val_loss: 0.6318 - val_accuracy: 0.2265\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0846 - accuracy: 0.2906 - val_loss: 0.6352 - val_accuracy: 0.2258\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0836 - accuracy: 0.2909 - val_loss: 0.6366 - val_accuracy: 0.2263\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0819 - accuracy: 0.2914 - val_loss: 0.6407 - val_accuracy: 0.2257\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0802 - accuracy: 0.2920 - val_loss: 0.6416 - val_accuracy: 0.2266\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0790 - accuracy: 0.2921 - val_loss: 0.6489 - val_accuracy: 0.2257\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0772 - accuracy: 0.2929 - val_loss: 0.6466 - val_accuracy: 0.2266\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0762 - accuracy: 0.2928 - val_loss: 0.6512 - val_accuracy: 0.2260\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0748 - accuracy: 0.2932 - val_loss: 0.6546 - val_accuracy: 0.2259\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0736 - accuracy: 0.2935 - val_loss: 0.6674 - val_accuracy: 0.2250\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0725 - accuracy: 0.2938 - val_loss: 0.6690 - val_accuracy: 0.2258\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0711 - accuracy: 0.2944 - val_loss: 0.6682 - val_accuracy: 0.2260\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0698 - accuracy: 0.2948 - val_loss: 0.6774 - val_accuracy: 0.2252\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0691 - accuracy: 0.2948 - val_loss: 0.6803 - val_accuracy: 0.2258\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0677 - accuracy: 0.2953 - val_loss: 0.6806 - val_accuracy: 0.2262\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0668 - accuracy: 0.2957 - val_loss: 0.6861 - val_accuracy: 0.2245\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0656 - accuracy: 0.2959 - val_loss: 0.6889 - val_accuracy: 0.2268\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0646 - accuracy: 0.2961 - val_loss: 0.6916 - val_accuracy: 0.2250\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0635 - accuracy: 0.2964 - val_loss: 0.6992 - val_accuracy: 0.2248\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0629 - accuracy: 0.2966 - val_loss: 0.7027 - val_accuracy: 0.2251\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0618 - accuracy: 0.2970 - val_loss: 0.7073 - val_accuracy: 0.2244\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0607 - accuracy: 0.2971 - val_loss: 0.7061 - val_accuracy: 0.2254\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0601 - accuracy: 0.2973 - val_loss: 0.7143 - val_accuracy: 0.2243\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0593 - accuracy: 0.2976 - val_loss: 0.7094 - val_accuracy: 0.2260\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0583 - accuracy: 0.2979 - val_loss: 0.7172 - val_accuracy: 0.2249\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0574 - accuracy: 0.2983 - val_loss: 0.7217 - val_accuracy: 0.2251\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0568 - accuracy: 0.2983 - val_loss: 0.7254 - val_accuracy: 0.2253\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0564 - accuracy: 0.2985 - val_loss: 0.7222 - val_accuracy: 0.2256\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0552 - accuracy: 0.2989 - val_loss: 0.7274 - val_accuracy: 0.2251\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0547 - accuracy: 0.2988 - val_loss: 0.7313 - val_accuracy: 0.2244\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0541 - accuracy: 0.2990 - val_loss: 0.7374 - val_accuracy: 0.2247\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0530 - accuracy: 0.2995 - val_loss: 0.7404 - val_accuracy: 0.2249\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 2s 13ms/step - loss: 0.0526 - accuracy: 0.2995 - val_loss: 0.7457 - val_accuracy: 0.2249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-wBPL4bRo_G"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJvLpDM8TGsc"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 인코더 모델에 입력 데이터를 넣어 상태값(h, c)을 계산\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # 디코더 모델에서 이전 스텝의 결과를 다음 스텝에 넣어주기 위한 변수 생성\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # 디코더 모델에 최초 스텝에 입력되는 시작 문자(Tab) 설정\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # 디코더에 의해 계산된 문자별 확률값에서 가장 높은 값의 인덱스를 가져옴\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # 반복문의 종료 조건 체크(스텝의 학습데이터의 최대 사이즈를 넘어가는지\n",
        "        # , 종료 문자(Tab) 발생하는지)\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 디코더의 다음 스텝에 입력데이터로 사용할 문자값을 계산된 값으로 업데이트\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # 디코더의 다음 스텝에 사용할 상테값을 계산된 값으로 업데이트\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keHCFjS8TJCF",
        "outputId": "e01afa62-7248-404b-c551-bcb1ea049258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for seq_index in range(100):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Va !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez !\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Qui ?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Ça ausez !\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Au feu !\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: À l'aide !\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Saute.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stop !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stop !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stop !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attends !\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Salut !\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Salut !\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Je ne me suis pas de le datson.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: J'essaye.\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Je l'ai emporté !\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Je l'ai emporté !\n",
            "\n",
            "-\n",
            "Input sentence: I won.\n",
            "Decoded sentence: J'ai paré.\n",
            "\n",
            "-\n",
            "Input sentence: Oh no!\n",
            "Decoded sentence: Oh non !\n",
            "\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: Attaquez !\n",
            "\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: Attaquez !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Tchin-tchin !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Tchin-tchin !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Tchin-tchin !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Tchin-tchin !\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Lève-toi.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Vas-y maintenant.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Vas-y maintenant.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Vas-y maintenant.\n",
            "\n",
            "-\n",
            "Input sentence: Got it!\n",
            "Decoded sentence: Compris !\n",
            "\n",
            "-\n",
            "Input sentence: Got it!\n",
            "Decoded sentence: Compris !\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris ?\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris ?\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris ?\n",
            "\n",
            "-\n",
            "Input sentence: Hop in.\n",
            "Decoded sentence: Monte.\n",
            "\n",
            "-\n",
            "Input sentence: Hop in.\n",
            "Decoded sentence: Monte.\n",
            "\n",
            "-\n",
            "Input sentence: Hug me.\n",
            "Decoded sentence: Serrez-moi dans vos bras !\n",
            "\n",
            "-\n",
            "Input sentence: Hug me.\n",
            "Decoded sentence: Serrez-moi dans vos bras !\n",
            "\n",
            "-\n",
            "Input sentence: I fell.\n",
            "Decoded sentence: Je suis tombé.\n",
            "\n",
            "-\n",
            "Input sentence: I fell.\n",
            "Decoded sentence: Je suis tombé.\n",
            "\n",
            "-\n",
            "Input sentence: I know.\n",
            "Decoded sentence: Je sais.\n",
            "\n",
            "-\n",
            "Input sentence: I left.\n",
            "Decoded sentence: Je suis partie.\n",
            "\n",
            "-\n",
            "Input sentence: I left.\n",
            "Decoded sentence: Je suis partie.\n",
            "\n",
            "-\n",
            "Input sentence: I lost.\n",
            "Decoded sentence: J'ai perdu mes chante.\n",
            "\n",
            "-\n",
            "Input sentence: I paid.\n",
            "Decoded sentence: J’ai payé.\n",
            "\n",
            "-\n",
            "Input sentence: I'm 19.\n",
            "Decoded sentence: J'ai la horte le fect.\n",
            "\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: Je suis reconnaissant.\n",
            "\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: Je suis reconnaissant.\n",
            "\n",
            "-\n",
            "Input sentence: Listen.\n",
            "Decoded sentence: Écoutez !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est pas possible !\n",
            "\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vrai ?\n",
            "\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vrai ?\n",
            "\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vrai ?\n",
            "\n",
            "-\n",
            "Input sentence: Thanks.\n",
            "Decoded sentence: Merci !\n",
            "\n",
            "-\n",
            "Input sentence: We try.\n",
            "Decoded sentence: On essaye.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'emportâmes.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'emportâmes.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'emportâmes.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous l'emportâmes.\n",
            "\n",
            "-\n",
            "Input sentence: Ask Tom.\n",
            "Decoded sentence: Demande à Tom.\n",
            "\n",
            "-\n",
            "Input sentence: Awesome!\n",
            "Decoded sentence: Fantastique !\n",
            "\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calmes !\n",
            "\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calmes !\n",
            "\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calmes !\n",
            "\n",
            "-\n",
            "Input sentence: Be cool.\n",
            "Decoded sentence: Sois détendu !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be kind.\n",
            "Decoded sentence: Sois gentil.\n",
            "\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille !\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d63986dc8076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-e6fa26223f58>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         output_tokens, h, c = decoder_model.predict(\n\u001b[0;32m---> 14\u001b[0;31m             [target_seq] + states_value)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# 디코더에 의해 계산된 문자별 확률값에서 가장 높은 값의 인덱스를 가져옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYgq5N85TlTt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}