{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faq-sol의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RNMmJjEzVg5"
      },
      "source": [
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_yER_Yzb37X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06704396-69b9-4ee9-d026-6a9d82934fdf"
      },
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=1LPUbPd_cBSJ9AFcH79er9uzj7JKFoZzj\" -O \"KorQuAD_v1.0_train.csv\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-11 06:22:15--  https://drive.google.com/uc?export=download&id=1LPUbPd_cBSJ9AFcH79er9uzj7JKFoZzj\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.76.113, 173.194.76.139, 173.194.76.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.76.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ogc989tp7lmdumk65bkv5ite6i06e16l/1605075675000/13472183013488020071/*/1LPUbPd_cBSJ9AFcH79er9uzj7JKFoZzj?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-11-11 06:22:17--  https://doc-08-94-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ogc989tp7lmdumk65bkv5ite6i06e16l/1605075675000/13472183013488020071/*/1LPUbPd_cBSJ9AFcH79er9uzj7JKFoZzj?e=download\n",
            "Resolving doc-08-94-docs.googleusercontent.com (doc-08-94-docs.googleusercontent.com)... 64.233.166.132, 2a00:1450:400c:c09::84\n",
            "Connecting to doc-08-94-docs.googleusercontent.com (doc-08-94-docs.googleusercontent.com)|64.233.166.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘KorQuAD_v1.0_train.csv’\n",
            "\n",
            "KorQuAD_v1.0_train.     [  <=>               ]   5.70M  18.1MB/s    in 0.3s    \n",
            "\n",
            "2020-11-11 06:22:17 (18.1 MB/s) - ‘KorQuAD_v1.0_train.csv’ saved [5981229]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pZftyOdjhKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d18a00-1f3f-422a-be62-96a22cc41711"
      },
      "source": [
        "# matplotlib에 한글 폰트 적용을 위해 아래 프로그램 설치 후 런타임 다시시작 필요\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 31 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR0BhvHincsu"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYzMdvb-ncsw"
      },
      "source": [
        "num_samples = 50000  # 입력 데이터의 최대 sequence.\n",
        "# 한글 QnA 데이터 셋 파일 경로\n",
        "data_path = '/content/KorQuAD_v1.0_train.csv'\n",
        "chekpoint_path = '/content/training_checkpoints' + time.strftime('%Y-%m-%d %H:%M:%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhbvVMeuncsy"
      },
      "source": [
        "mecab = Mecab()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohuAmn0L9m-H"
      },
      "source": [
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSg0Wlq69uak"
      },
      "source": [
        "# 데이터 분석에 불필요한 정보 제거\n",
        "def normalizeString(s):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ가-힣 ^☆; ^a-zA-Z.!?]+')\n",
        "    match = hangul.search(s)\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    if not match:\n",
        "      result = hangul.sub('', s)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvg22gTiA7R-"
      },
      "source": [
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTpM3xO0Clvf"
      },
      "source": [
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    tmp_text = line.split('\\t')\n",
        "    \n",
        "    if len(tmp_text) > 1:\n",
        "        input_text = normalizeString(tmp_text[0])\n",
        "        target_text = normalizeString(tmp_text[1])\n",
        "        \n",
        "    # \"tab\"을 목표 데이터의 시작, 종료 문자로 지정\n",
        "    if len(input_text) > 0 and len(target_text) > 0:\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnyXoLgPB7qJ"
      },
      "source": [
        "input_token_sequences = [mecab.morphs(sentence) for sentence in input_texts]\n",
        "target_token_sequences = [['\\t'] + mecab.morphs(sentence) + ['\\n'] for sentence in target_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn4Esyi4DyDb"
      },
      "source": [
        "input_tokenizer = Tokenizer(oov_token = 'OOV')\n",
        "input_tokenizer.fit_on_texts(input_token_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqAm0UDJD1cM"
      },
      "source": [
        "target_tokenizer = Tokenizer(oov_token = 'OOV')\n",
        "target_tokenizer.fit_on_texts(target_token_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA21vqdSFocz"
      },
      "source": [
        "input_sequences = input_tokenizer.texts_to_sequences(input_token_sequences)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_token_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jad5K9brncs4"
      },
      "source": [
        "num_encoder_tokens = len(input_tokenizer.word_index)\n",
        "num_decoder_tokens = len(target_tokenizer.word_index)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_sequences])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_sequences])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c95JiEWncs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12b186a-4417-4cf6-9c09-78db02a132e2"
      },
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 24754\n",
            "Number of unique input tokens: 22465\n",
            "Number of unique output tokens: 16266\n",
            "Max sequence length for inputs: 55\n",
            "Max sequence length for outputs: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4NMv0b3Plb2"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_sequences), max_encoder_seq_length),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(target_sequences), max_decoder_seq_length),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(target_sequences), max_decoder_seq_length),\n",
        "    dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF3m_pIXJuMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75694e82-c6c4-414a-e58c-9223dedbdb8f"
      },
      "source": [
        "input_sequences[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1647, 5, 9592, 4, 5476, 12, 1384, 25, 14, 6, 435, 282, 33, 15, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj_wIaUvnctC"
      },
      "source": [
        "for i, (input_id, target_id) in enumerate(zip(input_sequences, target_sequences)):\n",
        "    for t, id in enumerate(input_id):\n",
        "        encoder_input_data[i, t] = id\n",
        "    for t, id in enumerate(target_id):\n",
        "        decoder_input_data[i, t] = id\n",
        "        if t > 0:\n",
        "            # 디코더의 목표 데이터는 디코더 입력 데이터 보다 한 step 만큼 앞서 있음\n",
        "            # 또한 디코더의 목표 데이터는 시작 문자(tab) 이 존재하지 않음\n",
        "            decoder_target_data[i, t - 1] = id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-2OcnDOnctE"
      },
      "source": [
        "decoder_target_data = decoder_target_data.astype(np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIGl-iqglMKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d335fe-4f47-41d0-cf90-c0634da82728"
      },
      "source": [
        "input_tensor_train, input_tensor_val, \\\n",
        "target_tensor_train, target_tensor_val = train_test_split(encoder_input_data,\n",
        "                                                          decoder_target_data,\n",
        "                                                          test_size=0.2)\n",
        "\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19803, 19803, 4951, 4951)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdwRRxcLnctK"
      },
      "source": [
        "EPOCHS         = 20    # 학습하고자 하는 에폭 사이즈.\n",
        "BUFFER_SIZE    = len(input_tensor_train)\n",
        "BATCH_SIZE     = 32    \n",
        "N_BATCH        = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim  = 256   # word 임베딩의 사이즈 \n",
        "units          = 1024  # 인코더 디코더 모델의 unit 사이즈.\n",
        "vocab_inp_size = num_encoder_tokens + 1 # Toknizer 에서 0 인덱스는 비워두고 1번 부터 시작\n",
        "vocab_tar_size = num_decoder_tokens + 1 # Toknizer 에서 0 인덱스는 비워두고 1번 부터 시작\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiNwMbrLnctO"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
        "                                 return_sequences=True, \n",
        "                                 return_state=True, \n",
        "                                 recurrent_activation='sigmoid', \n",
        "                                 recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycXuStXXnctQ"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, \n",
        "                                 return_sequences=True, \n",
        "                                 return_state=True, \n",
        "                                 recurrent_activation='sigmoid', \n",
        "                                 recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output shape == (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beWNmWRwnctS"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVh9PupCnctW"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlOibaLPnctZ"
      },
      "source": [
        "checkpoint_prefix = os.path.join(chekpoint_path, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbXJXZRcnctc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38995257-c799-4ec9-c464-f3b38db24558"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            dec_input = tf.expand_dims([target_tokenizer.word_index['\\t']] * BATCH_SIZE,\n",
        "                                       axis=1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(0, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.2946\n",
            "Epoch 1 Batch 100 Loss 0.9616\n",
            "Epoch 1 Batch 200 Loss 0.8738\n",
            "Epoch 1 Batch 300 Loss 0.8745\n",
            "Epoch 1 Batch 400 Loss 0.9260\n",
            "Epoch 1 Batch 500 Loss 1.0409\n",
            "Epoch 1 Batch 600 Loss 0.9290\n",
            "Epoch 1 Loss 0.9483\n",
            "Time taken for 1 epoch 335.2154612541199 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8341\n",
            "Epoch 2 Batch 100 Loss 0.8416\n",
            "Epoch 2 Batch 200 Loss 0.8342\n",
            "Epoch 2 Batch 300 Loss 0.8898\n",
            "Epoch 2 Batch 400 Loss 0.7056\n",
            "Epoch 2 Batch 500 Loss 0.8068\n",
            "Epoch 2 Batch 600 Loss 0.9328\n",
            "Epoch 2 Loss 0.8743\n",
            "Time taken for 1 epoch 335.84636545181274 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.8467\n",
            "Epoch 3 Batch 100 Loss 0.7767\n",
            "Epoch 3 Batch 200 Loss 0.8302\n",
            "Epoch 3 Batch 300 Loss 0.7897\n",
            "Epoch 3 Batch 400 Loss 0.9014\n",
            "Epoch 3 Batch 500 Loss 0.8182\n",
            "Epoch 3 Batch 600 Loss 0.8360\n",
            "Epoch 3 Loss 0.8535\n",
            "Time taken for 1 epoch 335.5973858833313 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8038\n",
            "Epoch 4 Batch 100 Loss 0.7920\n",
            "Epoch 4 Batch 200 Loss 0.8466\n",
            "Epoch 4 Batch 300 Loss 0.9448\n",
            "Epoch 4 Batch 400 Loss 0.7688\n",
            "Epoch 4 Batch 500 Loss 0.9675\n",
            "Epoch 4 Batch 600 Loss 1.0079\n",
            "Epoch 4 Loss 0.8757\n",
            "Time taken for 1 epoch 335.4421591758728 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.9272\n",
            "Epoch 5 Batch 100 Loss 0.7756\n",
            "Epoch 5 Batch 200 Loss 0.9554\n",
            "Epoch 5 Batch 300 Loss 0.9406\n",
            "Epoch 5 Batch 400 Loss 0.9733\n",
            "Epoch 5 Batch 500 Loss 0.7157\n",
            "Epoch 5 Batch 600 Loss 0.8754\n",
            "Epoch 5 Loss 0.8800\n",
            "Time taken for 1 epoch 335.7709581851959 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8481\n",
            "Epoch 6 Batch 100 Loss 0.9191\n",
            "Epoch 6 Batch 200 Loss 1.0040\n",
            "Epoch 6 Batch 300 Loss 0.9261\n",
            "Epoch 6 Batch 400 Loss 0.9188\n",
            "Epoch 6 Batch 500 Loss 0.9326\n",
            "Epoch 6 Batch 600 Loss 0.9542\n",
            "Epoch 6 Loss 0.9286\n",
            "Time taken for 1 epoch 336.5849096775055 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7693\n",
            "Epoch 7 Batch 100 Loss 0.8712\n",
            "Epoch 7 Batch 200 Loss 0.9064\n",
            "Epoch 7 Batch 300 Loss 0.7764\n",
            "Epoch 7 Batch 400 Loss 0.7176\n",
            "Epoch 7 Batch 500 Loss 0.8058\n",
            "Epoch 7 Batch 600 Loss 0.9065\n",
            "Epoch 7 Loss 0.8663\n",
            "Time taken for 1 epoch 334.3285186290741 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8447\n",
            "Epoch 8 Batch 100 Loss 0.7461\n",
            "Epoch 8 Batch 200 Loss 0.7643\n",
            "Epoch 8 Batch 300 Loss 0.8439\n",
            "Epoch 8 Batch 400 Loss 0.7559\n",
            "Epoch 8 Batch 500 Loss 0.8104\n",
            "Epoch 8 Batch 600 Loss 0.8144\n",
            "Epoch 8 Loss 0.8130\n",
            "Time taken for 1 epoch 335.35901641845703 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.7090\n",
            "Epoch 9 Batch 100 Loss 0.7814\n",
            "Epoch 9 Batch 200 Loss 0.7815\n",
            "Epoch 9 Batch 300 Loss 0.8079\n",
            "Epoch 9 Batch 400 Loss 0.8932\n",
            "Epoch 9 Batch 500 Loss 0.7453\n",
            "Epoch 9 Batch 600 Loss 0.9231\n",
            "Epoch 9 Loss 0.8092\n",
            "Time taken for 1 epoch 334.2503309249878 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8656\n",
            "Epoch 10 Batch 100 Loss 0.7287\n",
            "Epoch 10 Batch 200 Loss 0.7559\n",
            "Epoch 10 Batch 300 Loss 0.7890\n",
            "Epoch 10 Batch 400 Loss 0.6814\n",
            "Epoch 10 Batch 500 Loss 0.8371\n",
            "Epoch 10 Batch 600 Loss 0.7752\n",
            "Epoch 10 Loss 0.7997\n",
            "Time taken for 1 epoch 339.5621042251587 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.6754\n",
            "Epoch 11 Batch 100 Loss 0.6084\n",
            "Epoch 11 Batch 200 Loss 0.7084\n",
            "Epoch 11 Batch 300 Loss 0.8403\n",
            "Epoch 11 Batch 400 Loss 0.6826\n",
            "Epoch 11 Batch 500 Loss 0.7158\n",
            "Epoch 11 Batch 600 Loss 0.7516\n",
            "Epoch 11 Loss 0.7049\n",
            "Time taken for 1 epoch 335.80326628685 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.6195\n",
            "Epoch 12 Batch 100 Loss 0.5956\n",
            "Epoch 12 Batch 200 Loss 0.6079\n",
            "Epoch 12 Batch 300 Loss 0.6028\n",
            "Epoch 12 Batch 400 Loss 0.6262\n",
            "Epoch 12 Batch 500 Loss 0.8218\n",
            "Epoch 12 Batch 600 Loss 0.6775\n",
            "Epoch 12 Loss 0.6502\n",
            "Time taken for 1 epoch 334.11200618743896 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.5389\n",
            "Epoch 13 Batch 100 Loss 0.6132\n",
            "Epoch 13 Batch 200 Loss 0.6235\n",
            "Epoch 13 Batch 300 Loss 0.5734\n",
            "Epoch 13 Batch 400 Loss 0.5489\n",
            "Epoch 13 Batch 500 Loss 0.6694\n",
            "Epoch 13 Batch 600 Loss 0.5112\n",
            "Epoch 13 Loss 0.5839\n",
            "Time taken for 1 epoch 332.26064467430115 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.6058\n",
            "Epoch 14 Batch 100 Loss 0.4646\n",
            "Epoch 14 Batch 200 Loss 0.5726\n",
            "Epoch 14 Batch 300 Loss 0.4792\n",
            "Epoch 14 Batch 400 Loss 0.4716\n",
            "Epoch 14 Batch 500 Loss 0.5186\n",
            "Epoch 14 Batch 600 Loss 0.6323\n",
            "Epoch 14 Loss 0.4983\n",
            "Time taken for 1 epoch 332.5915422439575 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.4026\n",
            "Epoch 15 Batch 100 Loss 0.3083\n",
            "Epoch 15 Batch 200 Loss 0.4019\n",
            "Epoch 15 Batch 300 Loss 0.4804\n",
            "Epoch 15 Batch 400 Loss 0.3299\n",
            "Epoch 15 Batch 500 Loss 0.4093\n",
            "Epoch 15 Batch 600 Loss 0.3689\n",
            "Epoch 15 Loss 0.3990\n",
            "Time taken for 1 epoch 329.85851097106934 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.2494\n",
            "Epoch 16 Batch 100 Loss 0.2834\n",
            "Epoch 16 Batch 200 Loss 0.2993\n",
            "Epoch 16 Batch 300 Loss 0.2596\n",
            "Epoch 16 Batch 400 Loss 0.3035\n",
            "Epoch 16 Batch 500 Loss 0.3210\n",
            "Epoch 16 Batch 600 Loss 0.3397\n",
            "Epoch 16 Loss 0.2987\n",
            "Time taken for 1 epoch 330.3437976837158 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1947\n",
            "Epoch 17 Batch 100 Loss 0.1557\n",
            "Epoch 17 Batch 200 Loss 0.2387\n",
            "Epoch 17 Batch 300 Loss 0.2478\n",
            "Epoch 17 Batch 400 Loss 0.2457\n",
            "Epoch 17 Batch 500 Loss 0.2314\n",
            "Epoch 17 Batch 600 Loss 0.2127\n",
            "Epoch 17 Loss 0.2098\n",
            "Time taken for 1 epoch 331.27096247673035 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1741\n",
            "Epoch 18 Batch 100 Loss 0.1611\n",
            "Epoch 18 Batch 200 Loss 0.1850\n",
            "Epoch 18 Batch 300 Loss 0.1282\n",
            "Epoch 18 Batch 400 Loss 0.1420\n",
            "Epoch 18 Batch 500 Loss 0.1472\n",
            "Epoch 18 Batch 600 Loss 0.1328\n",
            "Epoch 18 Loss 0.1402\n",
            "Time taken for 1 epoch 335.3734676837921 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1038\n",
            "Epoch 19 Batch 100 Loss 0.0751\n",
            "Epoch 19 Batch 200 Loss 0.1000\n",
            "Epoch 19 Batch 300 Loss 0.0876\n",
            "Epoch 19 Batch 400 Loss 0.0531\n",
            "Epoch 19 Batch 500 Loss 0.1127\n",
            "Epoch 19 Batch 600 Loss 0.1065\n",
            "Epoch 19 Loss 0.0909\n",
            "Time taken for 1 epoch 335.4394142627716 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0815\n",
            "Epoch 20 Batch 100 Loss 0.0439\n",
            "Epoch 20 Batch 200 Loss 0.0461\n",
            "Epoch 20 Batch 300 Loss 0.0527\n",
            "Epoch 20 Batch 400 Loss 0.0308\n",
            "Epoch 20 Batch 500 Loss 0.0731\n",
            "Epoch 20 Batch 600 Loss 0.0654\n",
            "Epoch 20 Loss 0.0577\n",
            "Time taken for 1 epoch 335.84967708587646 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}