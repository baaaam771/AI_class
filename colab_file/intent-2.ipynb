{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intent",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u9rzL83Ie3T"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 데이터 분석에 불필요한 정보 제거\n",
        "def normalizeString(s):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ가-힣 ^☆; ^a-zA-Z~.!?]+')\n",
        "    match = hangul.search(s)\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if not match:\n",
        "        result = hangul.sub('', s)\n",
        "\n",
        "    return result\n",
        "\n",
        "def preprocess_data(data_path):\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "\n",
        "    input_texts = []\n",
        "    train_labels = []\n",
        "    input_chars = set()\n",
        "\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "\n",
        "    for line in lines[:2500]:\n",
        "        tmp_text = line.split('\\t')\n",
        "\n",
        "        if len(tmp_text) > 1:\n",
        "            input_text = normalizeString(tmp_text[0])\n",
        "            target_label = tmp_text[1]\n",
        "\n",
        "        if len(input_text) > 0 and len(target_label) > 0:\n",
        "            input_texts.append(input_text)\n",
        "            train_labels.append(target_label)\n",
        "            for char in input_text:\n",
        "                if char not in input_chars:\n",
        "                    input_chars.add(char)\n",
        "\n",
        "    input_chars = sorted(list(input_chars))\n",
        "    num_input_tokens = len(input_chars)\n",
        "    num_sequence = max([len(txt) for txt in input_texts])\n",
        "\n",
        "    print('Number of samples:', len(input_texts))\n",
        "    print('Number of unique input tokens:', num_input_tokens)\n",
        "    print('Max sequence length for inputs:', num_sequence)\n",
        "\n",
        "    input_token_index = dict(\n",
        "        [(char, i) for i, char in enumerate(input_chars)])\n",
        "\n",
        "    train_data = np.zeros((len(input_texts),\n",
        "                           num_sequence),\n",
        "                          dtype='float32')\n",
        "\n",
        "    for i, input_text in enumerate(input_texts):\n",
        "        for t, char in enumerate(input_text):\n",
        "            train_data[i, t] = input_token_index[char]\n",
        "\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    return train_data, train_labels, num_sequence, num_input_tokens, input_token_index"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTBwND9ZJm-f"
      },
      "source": [
        "def normalizeString(s):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ가-힣 ^☆; ^a-zA-Z.!?]+')\n",
        "    match = hangul.search(s)\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if not match:\n",
        "        result = hangul.sub('', s)\n",
        "\n",
        "    return result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OVV69vYI23J"
      },
      "source": [
        "def preprocess_data():\n",
        "\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "\n",
        "  with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "  for line in lines[:2500]:\n",
        "    tmp_txt = line.split('\\t')\n",
        "\n",
        "    if len(tmp_txt) > 1:\n",
        "      input_text = normalizeStrring(tmp_txt[0])\n",
        "      target_text = normalizeStrring(tmp_txt[1])\n",
        "    if len(input_text) > 0 and len(target_text) > 0:\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "\n",
        "  return input_texts, target_texts"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1sBXnoEJmbU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}