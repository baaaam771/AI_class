{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import BertTokenizer,BertConfig,TFBertModel\nfrom tqdm import tqdm","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/tweet-sentiment-extraction/'\ntrain_df = pd.read_csv(DATA_PATH + 'train.csv')\ntest_df = pd.read_csv(DATA_PATH + 'test.csv')\nsubmission_df = pd.read_csv(DATA_PATH + 'sample_submission.csv')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 32\n    TEST_BATCH_SIZE = 32\n    EPOCHS = 5\n    BERT_CONFIG = '/kaggle/input/bert-base-uncased-config.json'\n    BERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer/'\n    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n        f'{BERT_PATH}/bert-base-uncased-vocab.txt',\n        lowercase=True\n    )\n    SAVEMODEL_PATH = '/kaggle/input/tftweet/finetuned_bert.h5'\n    THRESHOLD = 0.4\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, tokenizer):\n    len_st = len(selected_text)\n    idx0 = None\n    idx1 = None\n    \n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind:ind+len_st] == selected_text:\n            idx0 = ind\n            idx1= ind + len_st\n            break\n            \n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1):\n            char_targets[ct] = 1\n            \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for i, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1:offset2]) > 0:\n            target_idx.append(i)\n            \n    targets = [0] * len(input_ids_orig)\n    \n    for idx in target_idx:\n        targets[idx] = 1\n        \n    return targets","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['targets'] = train_df.apply(lambda row: process_data(\n                                                                    str(row['text']), \n                                                                    str(row['selected_text']),\n                                                                    config.TOKENIZER),\n                                                                    axis=1)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['targets'] = train_df['targets'].apply(lambda x: x + [0] * (config.MAX_LEN - len(x)))","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _convert_to_transformer_inputs(text, tokenizer, max_sequence_length):\n    inputs = tokenizer.encode(text)\n    input_ids =  inputs.ids\n    input_masks = inputs.attention_mask\n    input_segments = inputs.type_ids\n    padding_length = max_sequence_length - len(input_ids)\n    padding_id = 0\n    input_ids = input_ids + ([padding_id] * padding_length)\n    input_masks = input_masks + ([0] * padding_length)\n    input_segments = input_segments + ([0] * padding_length)\n    return [input_ids, input_masks, input_segments]\n\n","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef compute_input_arrays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df.iterrows()):\n        ids, masks, segments= _convert_to_transformer_inputs(str(instance.text),tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns].values.tolist())","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays(train_df, 'targets')\ninputs = compute_input_arrays(train_df, config.TOKENIZER, config.MAX_LEN)\ntest_inputs = compute_input_arrays(test_df, config.TOKENIZER, config.MAX_LEN)","execution_count":41,"outputs":[{"output_type":"stream","text":"27481it [00:08, 3225.24it/s]\n3534it [00:00, 3677.32it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN, ), dtype=tf.int32)\n    mask = tf.keras.layers.Input((config.MAX_LEN, ), dtype=tf.int32)\n    attention = tf.keras.layers.Input((config.MAX_LEN, ), dtype=tf.int32)\n    bert_config = BertConfig()\n    bert_model = TFBertModel.from_pretrained(config.BERT_PATH + '/bert-base-uncased-tf_model.h5'\n                                            , config=bert_config)\n    \n    output = bert_model(ids, attention_mask=mask, token_type_ids=attention)\n    \n    out = tf.keras.layers.Dropout(0.1)(output[0])\n    out = tf.keras.layers.Conv1D(1,1)(out)\n    out = tf.keras.layers.Flatten()(out)\n    out = tf.keras.layers.Activation('sigmoid')(out)\n    model = tf.keras.models.Model(inputs=[ids, mask, attention], outputs=out)\n    \n    return model","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)","execution_count":43,"outputs":[{"output_type":"stream","text":"Some layers from the model checkpoint at /kaggle/input/bert-base-uncased-huggingface-transformer//bert-base-uncased-tf_model.h5 were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at /kaggle/input/bert-base-uncased-huggingface-transformer//bert-base-uncased-tf_model.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.exists(config.SAVEMODEL_PATH):\n    model.fit(x=inputs, y=outputs, epochs=config.EPOCHS, batch_size=config.TRAIN_BATCH_SIZE)\n    model.save_weights(f'finetuned_bert.h5')\nelse:\n    model.load_weights(config.SAVEMODEL_PATH)","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n430/430 [==============================] - 381s 885ms/step - loss: 0.0976\nEpoch 2/5\n 48/430 [==>...........................] - ETA: 5:32 - loss: 0.0758","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_outputs = compute_output_arrays(test_df, 'targets')\n\nmodel.evalute(test_inputs, test_outputs)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}