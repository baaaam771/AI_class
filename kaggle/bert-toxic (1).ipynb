{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport bert\nfrom tensorflow.keras.models import  Model\nfrom tqdm import tqdm\nimport numpy as np\nfrom collections import namedtuple\nfrom imblearn.over_sampling import RandomOverSampler\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport tensorflow_addons as tfa\n# multi-label hamming loss\nimport tensorflow_addons as tfa\nhl = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ_LEN=128\ninput_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n                                       name=\"input_word_ids\")\ninput_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n                                   name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n                                    name=\"segment_ids\")\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef custom_recall(y_true,y_pred):\n    y_true = tf.convert_to_tensor(y_true)\n    Y_PRED = tf.cast(y_pred>=0.4,tf.float32)\n    recall = tf.math.divide_no_nan(tf.math.reduce_sum(tf.math.multiply(y_true,Y_PRED)),tf.math.reduce_sum(y_true))\n    return recall\nx = tf.keras.layers.Dropout(0.1)(pooled_output)\nx= tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.1)(x)\nout = tf.keras.layers.Dense(6, activation=\"sigmoid\", name=\"dense_output\")(x)\nmodel = tf.keras.models.Model(\n      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=[hl,custom_recall])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FullTokenizer=bert.bert_tokenization.FullTokenizer\nvocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer=FullTokenizer(vocab_file,do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def results(pred_results,ground_truth,threshold):\n    hl = tfa.metrics.HammingLoss(mode='multilabel', threshold=threshold)\n    hl.update_state(ground_truth, pred_results)\n    print('Hamming loss:', hl.result().numpy())\n    pred_results = (pred_results>=threshold).astype(int)\n    print(\"Positive labels RECALL-(thre:{}):  {}%\".format(threshold,100*np.sum(pred_results*ground_truth)/np.sum(ground_truth)))\n    cf = tfa.metrics.MultiLabelConfusionMatrix(num_classes=6)\n    cf.update_state(ground_truth, pred_results)\n    print('Confusion matrix:', cf.result().numpy())\n    return \n\ndef RS(k,features_train,ratio=1.0):\n    train_y = features_train[:,3*128:390]\n    label = train_y[:,k]\n    rus = RandomOverSampler(ratio,random_state=42)\n    features_train_res, _ = rus.fit_resample(features_train, label)\n    np.random.shuffle(features_train_res)\n    features_train_res = features_train_res[:100000,:]\n    return features_train_res\n\ndef index_set(k,pattern):\n    m= 2**(5-k)\n    return set([i for i in range(64) if (i//m%2)!=0]).intersection(set(pattern.keys()))\n\ndef label_to_class(v):\n    w = np.array([32,16,8,4,2,1],dtype=np.int16)\n    return np.sum(v*w,axis=1)\n\ndef class_to_label(c):\n    c=np.reshape(c,(-1,1))\n    v5=c%2\n    c=c//2\n    v4=c%2\n    c=c//2\n    v3=c%2\n    c=c//2\n    v2=c%2\n    c=c//2\n    v1=c%2\n    c=c//2\n    v0=c%2\n    return np.concatenate([v0,v1,v2,v3,v4,v5],axis=1)\ndef distribution(pattern):\n    b=class_to_label(np.array(list(pattern.keys())))\n    a=np.asarray(list(pattern.values())).reshape(-1,1)\n    M = a*b\n    w_vec=np.sum(M,axis=0)\n    #return w_vec/np.sum(w_vec)\n    return w_vec\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_masks(tokens, max_seq_length):\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\ndef get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\ndef get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef create_single_input(sentence,MAX_LEN):\n    stokens = tokenizer.tokenize(sentence)\n    stokens = stokens[:MAX_LEN]\n    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n    ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n    masks = get_masks(stokens, MAX_SEQ_LEN)\n    segments = get_segments(stokens, MAX_SEQ_LEN)\n    return ids,masks,segments\ndef create_input_array(sentences):\n    input_ids, input_masks, input_segments = [], [], []\n    for sentence in tqdm(sentences,position=0, leave=True):\n        ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ndf = df.sample(frac=1)\ntrain_sentences = df[\"comment_text\"].fillna(\"CVxTz\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ninputs=create_input_array(train_sentences)\n\ntrain_y = df[list_classes].values\nfeatures=np.concatenate(inputs,axis=1)\nfeatures=np.concatenate((features,train_y),axis=1)\n\nfeatures_train,features_test,train_y,_= train_test_split(features, train_y,test_size=0.2,random_state=42)\nprint(\"Train set samples:\",train_y.shape[0])\n\nprint(\"Positive labels distribution:\",distribution(Counter(label_to_class(train_y)))/train_y.shape[0])\n\nprior = distribution(Counter(label_to_class(train_y)))/train_y.shape[0]\nlist_trains = []\nfeatures_train_res= RS(3,features_train)\nlist_trains.append(features_train_res)\nfeatures_train_res= RS(5,features_train)\nlist_trains.append(features_train_res)\nfeatures_train_res= RS(1,features_train)\nlist_trains.append(features_train_res)\nfeatures_train_res= RS(4,features_train)\nlist_trains.append(features_train_res)\nfeatures_train_res= RS(2,features_train)\nlist_trains.append(features_train_res)\nfeatures_train_res= RS(0,features_train)\nlist_trains.append(features_train_res)\n\nfeatures_train=np.concatenate(list_trains,axis=0)\nnp.random.shuffle(features_train)\ntrain_y = features_train[:,3*128:390]\nafter = distribution(Counter(label_to_class(train_y)))/600000\nprint(\"Positive sample amplification through resampling:\",after/prior)\nprint(\"Positive labels distribution after resampling:\",after)\n\nX = np.split(features_train,[128,2*128,3*128,390],axis=1)\ninput_ids=X[0]\ninput_masks=X[1]\ninput_segments=X[2]\ntrain_y= X[3].astype(np.float32)\n\nX = np.split(features_test,[128,2*128,3*128,390],axis=1)\ninput_ids_test=X[0]\ninput_masks_test=X[1]\ninput_segments_test=X[2]\ntest_y= X[3].astype(np.float32)\n\ninputs_train=[input_ids,input_masks,input_segments]\ninputs_test=[input_ids_test,input_masks_test,input_segments_test]\nprint(\"Test set samples:\",test_y.shape[0])\nprint(\"Train set samples after resampling:\",train_y.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(inputs_train,train_y,epochs=2,batch_size=64,shuffle=True)\nmodel.save_weights('/kaggle/working/chkpt')\n#model.load_weights('/kaggle/input/keras-bert/chkpt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_results = model.predict(inputs_test,batch_size=256)\nground_truth = test_y.astype(int)\nresults(pred_results,ground_truth,0.4)\nresults(pred_results,ground_truth,0.3)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}