# -*- coding: utf-8 -*-
"""multi-input_lstm_kor-Babi_faq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ddXQ27Gg2EbJNxJJQsxybHATxhwzorwf
"""

!wget "https://drive.google.com/uc?export=download&id=1xjztJzSPihlBIkgez1WutAdIoAP-D47S" -O "train.txt"
!wget "https://drive.google.com/uc?export=download&id=18WWa1gmPjyrI9N9jXDEPLnpA456sTdxu" -O "test.txt"

pip install customized_konlpy

from ckonlpy.tag import Twitter
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import numpy as np
from nltk import FreqDist
from functools import reduce
import os
import matplotlib.pyplot as plt

TRAIN_FILE = "train.txt"
TEST_FILE = "test.txt"

def read_data(dir):
    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정
    story_temp = [] # 현재 시점의 스토리 임시 저장
    lines = open(dir, "rb")

    for line in lines:
        line = line.decode("utf-8") # b' 제거
        line = line.strip() # '\n' 제거
        idx, text = line.split(" ", 1) # 맨 앞에 있는 id number 분리
        # 여기까지는 모든 줄에 적용되는 전처리

        if int(idx) == 1:
            story_temp = []

        if "\t" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우
            question, answer, _ = text.split("\t") # 질문과 답변을 각각 저장
            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장
            questions.append(question)
            answers.append(answer)

        else: # 현재 읽는 줄이 스토리인 경우
            story_temp.append(text) # 임시 저장

    lines.close()
    return stories, questions, answers

train_data = read_data(TRAIN_FILE)
test_data = read_data(TEST_FILE)

train_data[0][0]

train_data[1][0]

train_data[2][0]

train_stories, train_questions, train_answers = read_data(TRAIN_FILE)
test_stories, test_questions, test_answers = read_data(TEST_FILE)

print('훈련용 스토리의 개수 :', len(train_stories))
print('훈련용 질문의 개수 :',len(train_questions))
print('훈련용 답변의 개수 :',len(train_answers))
print('테스트용 스토리의 개수 :',len(test_stories))
print('테스트용 질문의 개수 :',len(test_questions))
print('테스트용 답변의 개수 :',len(test_answers))

train_stories[3572]

train_questions[3572]

train_answers[3572]

twitter = Twitter()

print(twitter.morphs('은경이는 화장실로 이동했습니다.'))
print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))
print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))
print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))
print(twitter.morphs('수종이는 사무실로 갔습니다.'))
print(twitter.morphs('은경이는 침실로 갔습니다.'))

twitter.add_dictionary('은경이', 'Noun')
twitter.add_dictionary('경임이', 'Noun')
twitter.add_dictionary('수종이', 'Noun')

print(twitter.morphs('은경이는 화장실로 이동했습니다.'))
print(twitter.morphs('경임이는 정원으로 가버렸습니다.'))
print(twitter.morphs('수종이는 복도로 뛰어갔습니다.'))
print(twitter.morphs('필웅이는 부엌으로 복귀했습니다.'))
print(twitter.morphs('수종이는 사무실로 갔습니다.'))
print(twitter.morphs('은경이는 침실로 갔습니다.'))

def preprocess_data(train_data, test_data):
    counter = FreqDist()

    # 두 문장의 story를 하나의 문장으로 통합하는 함수
    flatten = lambda data: reduce(lambda x, y: x + y, data)

    # 각 샘플의 길이를 저장하는 리스트
    story_len = []
    question_len = []

    for stories, questions, answers in [train_data, test_data]:
        for story in stories:
            story = twitter.morphs(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화
            story_len.append(len(story)) # 각 story의 길이 저장
            for word in story: # 단어 집합에 단어 추가
                counter[word] += 1
        for question in questions:
            question = twitter.morphs(question)
            question_len.append(len(question))
            for word in question:
                counter[word] += 1
        for answer in answers:
            answer = twitter.morphs(answer)
            for word in answer:
                counter[word] += 1

    # 단어 집합 생성
    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}
    idx2word = {idx : word for word, idx in word2idx.items()}

    # 가장 긴 샘플의 길이
    story_max_len = np.max(story_len)
    question_max_len = np.max(question_len)

    return word2idx, idx2word, story_max_len, question_max_len

word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)

print(word2idx)

vocab_size = len(word2idx) + 1
print(vocab_size)

print('스토리의 최대 길이 :',story_max_len)
print('질문의 최대 길이 :',question_max_len)

def vectorize(data, word2idx, story_maxlen, question_maxlen):
    Xs, Xq, Y = [], [], []
    flatten = lambda data: reduce(lambda x, y: x + y, data)

    stories, questions, answers = data
    for story, question, answer in zip(stories, questions, answers):
      xs = [word2idx[w] for w in twitter.morphs((flatten(story)))]
      xq = [word2idx[w] for w in twitter.morphs((question))]
      Xs.append(xs)
      Xq.append(xq)
      Y.append(word2idx[answer])

        # 스토리와 질문은 각각의 최대 길이로 패딩
        # 정답은 원-핫 인코딩
    return pad_sequences(Xs, maxlen=story_maxlen),\
           pad_sequences(Xq, maxlen=question_maxlen),\
           to_categorical(Y, num_classes=len(word2idx) + 1)

Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)
Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)

print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)

from tensorflow.keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras import Input

story_input = Input(shape=(story_max_len,), name='story')

embedded_story = layers.Embedding(vocab_size, 128)(story_input)
encoded_story = layers.LSTM(64)(embedded_story)

question_input = Input(shape=(question_max_len,), name='question')

embedded_question = layers.Embedding(vocab_size, 64)(question_input)
encoded_question = layers.LSTM(32)(embedded_question)

concatenated = layers.Concatenate(axis=-1)([encoded_story, encoded_question])

dense = layers.Dense(64, activation='relu')(concatenated)
dense = layers.Dropout(0.2)(dense)
answer = layers.Dense(vocab_size, activation='softmax')(dense)

model = Model(
    inputs=[story_input, question_input],
    outputs=answer
)

from tensorflow.keras.utils import plot_model

plot_model(model, show_shapes=True)

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x={'story':Xstrain,
                       'question':Xqtrain},
                    y=Ytrain,
                    batch_size=32,
                    epochs=120,
                    validation_split=0.2)

test_loss, test_accuracy = model.evaluate(x={'text':Xstest,
                                             'question':Xqtest},
                                          y=Ytest,)

# plot accuracy and loss plot
plt.subplot(211)
plt.title("Accuracy")
plt.plot(history.history["accuracy"], color="g", label="train")
plt.plot(history.history["val_accuracy"], color="b", label="validation")
plt.legend(loc="best")

plt.subplot(212)
plt.title("Loss")
plt.plot(history.history["loss"], color="g", label="train")
plt.plot(history.history["val_loss"], color="b", label="validation")
plt.legend(loc="best")

plt.tight_layout()
plt.show()

# labels
ytest = np.argmax(Ytest, axis=1)

# get predictions
Ytest_ = model.predict(x={'story' : Xstest,
                          'question' : Xqtest})
ytest_ = np.argmax(Ytest_, axis=1)

NUM_DISPLAY = 30

print("{:18}|{:5}|{}".format("질문", "실제값", "예측값"))
print(39 * "-")

for i in range(NUM_DISPLAY):
    question = " ".join([idx2word[x] for x in Xqtest[i].tolist()])
    label = idx2word[ytest[i]]
    prediction = idx2word[ytest_[i]]
    print("{:20}: {:7} {}".format(question, label, prediction))