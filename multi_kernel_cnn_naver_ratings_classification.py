# -*- coding: utf-8 -*-
"""multi-kernel-cnn_naver-ratings_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OMa3hETHzTy_5tqhmSG8gKjZRFh23LAx
"""

!set -x \
&& pip install konlpy \
&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

!wget "https://drive.google.com/uc?export=download&id=1ByJN0Vh4ctIwNdnO2jevEcBgrbRHuNyM" -O "ratings_train.txt"
!wget "https://drive.google.com/uc?export=download&id=1fNm-8pQJsuDbFaVIMow1DI-7lsnNLRFB" -O "ratings_test.txt"

train_data = pd.read_table('ratings_train.txt')
test_data = pd.read_table('ratings_test.txt')

train_data.head()

len(train_data)

from konlpy.tag import Mecab

mecab  = Mecab()

X_train = [mecab.morphs(sentence) for sentence in train_data['document']]

X_test = [mecab.morphs(sentence) for sentence in test_data['document']]

vocab_size = 21645

tokenizer = Tokenizer(vocab_size, oov_token='OOV') 
tokenizer.fit_on_texts(X_train)

X_train[0]

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

X_train[0]

y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])

drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]

len(drop_train)

# 빈 샘플들을 제거
X_train = np.delete(X_train, drop_train, axis=0)
y_train = np.delete(y_train, drop_train, axis=0)

len(X_train)

max_len = 80

X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate

embedding_dim = 128
batch_size = 256
dropout_prob = (0.5, 0.8)
num_filters = 128

"""### 기본 함수형 API를 사용하여 Multi 커널(3, 4, 5) Conv1D 블록 생성"""

sequence_input = Input(shape=(max_len,))
z = Embedding(vocab_size, embedding_dim, input_length=max_len, name="embedding")(sequence_input)
z = Dropout(dropout_prob[0])(z)

conv_a = Conv1D(num_filters,
                kernel_size=3,
                activation="relu", name='a_conv')(z)
conv_a = GlobalMaxPooling1D(name='a_pooling')(conv_a)

conv_b = Conv1D(num_filters,
                kernel_size=4,
                activation="relu", name='b_conv')(z)
conv_b = GlobalMaxPooling1D(name='b_pooling')(conv_b)

conv_c = Conv1D(num_filters,
                kernel_size=5,
                activation="relu", name='c_conv')(z)
conv_c = GlobalMaxPooling1D(name='c_pooling')(conv_c)

z = Concatenate()([conv_a, conv_b, conv_c])
z = Dropout(dropout_prob[1])(z)
z = Dense(128, activation="relu")(z)
model_output = Dense(1, activation="sigmoid")(z)

model = Model(
    inputs=sequence_input,
    outputs=model_output
)

from tensorflow.keras.utils import plot_model

plot_model(model, show_shapes=True)

model.compile(optimizer='adam',
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])

history = model.fit(X_train, y_train,
                    epochs=20,
                    batch_size=batch_size,
                    validation_data=(X_test, y_test))

"""### for 문을 이용하여 Multi 커널(3, 4, 5) Conv1D 블록 생성"""

sequence_input = Input(shape=(max_len,))
z = Embedding(vocab_size, embedding_dim, input_length=max_len, name="embedding")(sequence_input)
z = Dropout(dropout_prob[0])(z)

conv_blocks = []

for idx, sz in enumerate([3, 4, 5]):
  conv = Conv1D(num_filters,
                kernel_size=sz,
                activation="relu",
                name=str(idx)+'_conv1D')(z)
  conv = GlobalMaxPooling1D(name=str(idx)+'_pooling')(conv)  
  conv_blocks.append(conv)

z = Concatenate()(conv_blocks)
z = Dropout(dropout_prob[1])(z)
z = Dense(128, activation="relu")(z)
model_output = Dense(1, activation="sigmoid")(z)

loop_model = Model(
    inputs=sequence_input,
    outputs=model_output
)

from tensorflow.keras.utils import plot_model

plot_model(loop_model, show_shapes=True)

loop_model.compile(optimizer='adam',
                   loss = 'binary_crossentropy',
                   metrics = ['accuracy'])

history = loop_model.fit(X_train, y_train,
                         epochs=20,
                         batch_size=batch_size,
                         validation_split=0.2)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

test_loss, test_accuracy = loop_model.evaluate(X_test, y_test)